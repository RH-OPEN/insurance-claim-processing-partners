{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {},
   "source": [
    "## LLMをプログラム的に操作する\n",
    "\n",
    "これまでに ChatGPT のような大規模言語モデル (LLM) とやり取りしたことがあるはずです。これは通常、UI またはアプリケーションを通じて行われます。\n",
    "\n",
    "このノートブックでは、Python を使用して、API を介して直接 LLM に接続し、クエリを実行します。このラボでは、**OLLAMA Mistral-7B** モデルを選択しました。[(https://ollama.com/library/mistral](https://ollama.com/library/mistral)). これは完全にオープン ソース モデル (Apache 2.0 ライセンス) であり、他の商用モデルやオープン ソース モデルよりもはるかに軽量ですが、特に私たちが使用しようとしているタスクでは非常に優れています。\n",
    "\n",
    "このモデルは既にラボ クラスターにデプロイされています。これは、モデルが小さい場合でも GPU ではなく CPU でホストしているため、LLM サービスの速度は低下しますが、デモ環境用の低コスト オプションが提供されるためです。理想的には、少なくとも 24 GB の RAM を搭載した GPU が必要になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd03df78-4cf2-43d8-9d99-b6e8c773cc3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain==0.1.14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### 要件とインポート\n",
    "\n",
    "ラボの指示に従って起動する適切なワークベンチ イメージを選択した場合は、必要なライブラリがすべてすでに用意されているはずです。そうでない場合は、次のセルの最初の行のコメントを解除して、適切なパッケージをすべてインストールします。その後、必要なライブラリをインポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchain\n",
    "\n",
    "Langchain (https://www.langchain.com/) は、言語モデルを利用したアプリケーションを開発するためのフレームワークです。LLM を適切にクエリするために手動で記述する必要のあるすべての定型コードを処理します。\n",
    "\n",
    "まず、LLM API をクエリできる場所とモデルに適用されるいくつかのパラメータで定義される **llm** インスタンスを作成します。たとえば、`max_new_tokens` は、モデルに最大 96 個のトークン (単語または単語の一部) で応答するように指示します。ここで非常に低く設定されている `temperature` は、モデルに真実に基づいたままで、あまり \"創造的\" にならないように指示します。結局のところ、ここでは派手な詩を書こうとしているわけではありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c36cbcd-889a-4e62-886c-bc4d2f60ba2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_server_url = \"http://llm.ic-shared-llm.svc.cluster.local:11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOllama\u001b[0m\n",
      "Params: {'model': 'mistral', 'format': None, 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': None, 'num_thread': None, 'num_predict': 512, 'repeat_last_n': None, 'repeat_penalty': 1.03, 'temperature': 0.01, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': 0.92}, 'system': None, 'template': None, 'keep_alive': None}\n"
     ]
    }
   ],
   "source": [
    "# LLM definition\n",
    "llm = Ollama(\n",
    "    base_url=inference_server_url,\n",
    "    model=\"mistral\",\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    num_predict=512,\n",
    "    repeat_penalty=1.03,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "また、モデルに送信するすべてのリクエスト (\"Prompt\") に適用する **テンプレート** も必要です。\n",
    "\n",
    "モデルにクエリを実行する場合、ユーザーが入力した内容をそのまま送信することはほとんどないでしょう。この入力に加えて、モデルがそれをどのように処理するかを認識できるように、適切な指示を与える必要があります。何をどのように回答するか、何に回答してはいけないか、どのような口調で答えるかなどです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8bb7517-faa2-43ed-a95d-835de975f916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template=\"\"\"<s>[INST]<<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always be as helpful as possible, while being safe.\n",
    "You will be asked a question, to which you must give an answer.\n",
    "Your answer should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\n",
    "If you don't know the answer to a question, answer \"I don't know\".\n",
    "<</SYS>>\n",
    "\n",
    "### QUESTION:\n",
    "{input}\n",
    "\n",
    "### ANSWER:\n",
    "[/INST]\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(input_variables=[\"input\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe2119-2128-4432-aed1-126e9c8c034f",
   "metadata": {},
   "source": [
    "Langchain を使用すると、これらの要素を簡単に \"つなぎ合わせ\"、モデルのクエリに使用する **会話** オブジェクトを作成できるようになりました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e6d9f32-d4ae-4c2f-b513-d520413d2cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversation = LLMChain(llm=llm,\n",
    "                        prompt=PROMPT,\n",
    "                        verbose=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "これで、モデルをクエリする準備ができました。クエリからの出力を待ちます。LLM サービスでは GPU ではなく CPU を使用しているため、1 ～ 2 分かかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca714bca-7cec-4afc-b275-fa389c05a993",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and learn like humans. Rather than being limited by their pre-programmed instructions, AI systems are designed to adapt and improve from experience. They can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. AI systems can be categorized as either narrow or general. Narrow AI is designed to perform a specific task, such as playing chess or recognizing speech, while general AI, also known as artificial general intelligence, has the ability to understand, learn, and apply knowledge across a wide range of tasks at a level equal to or beyond a human being."
     ]
    }
   ],
   "source": [
    "query = \"What is Artificial Intelligence?\"\n",
    "conversation.predict(input=query); # \";\" at the end of the line hides final output (repetion of the streamed answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
